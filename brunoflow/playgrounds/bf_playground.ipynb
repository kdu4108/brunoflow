{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76f1067-b07e-45e5-afc4-f44cd4260460",
   "metadata": {},
   "source": [
    "# Compare Brunoflow to Pytorch and validate it works as expected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda04138-8ddf-4b93-aca6-b10e03d675e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import brunoflow as bf\n",
    "from brunoflow import Node\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca26c95-078b-4a12-ba47-f6f7cb989d0c",
   "metadata": {},
   "source": [
    "### Simple scalar math operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad47582-83a5-4724-82a3-4d68b7ce68bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results: 1026 10.0 300.0\n",
      "PT results: 1026.0 tensor(10.) tensor(300.)\n"
     ]
    }
   ],
   "source": [
    "# Brunoflow\n",
    "x_bf = bf.Parameter(5, name=\"x\")\n",
    "y_bf = bf.Parameter(10, name=\"y\")\n",
    "a_bf = bf.Parameter(1, name=\"a\")\n",
    "z_bf = x_bf * x_bf + y_bf * y_bf * y_bf + a_bf\n",
    "# z_bf.name = \"z\"\n",
    "z_bf.backprop()\n",
    "print(\"BF results:\", z_bf.val, x_bf.grad, y_bf.grad)\n",
    "\n",
    "# Pytorch\n",
    "x_pt = torch.tensor(5, dtype=torch.float32, requires_grad=True)\n",
    "y_pt = torch.tensor(10, dtype=torch.float32, requires_grad=True)\n",
    "a_pt = torch.tensor(1, dtype=torch.float32, requires_grad=True)\n",
    "z_pt = x_pt * x_pt + y_pt * y_pt * y_pt + a_pt\n",
    "z_pt.backward()\n",
    "print(\"PT results:\", z_pt.item(), x_pt.grad, y_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a0fc9-4fe1-41b6-a9b3-76d09ec59687",
   "metadata": {},
   "source": [
    "### Simple array math operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae083c8-ca98-4097-b7b2-e329026ce11b",
   "metadata": {},
   "source": [
    "#### Simplest case\n",
    "$$z = x^2 + y^3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bdee3-ed7e-4bd8-b0e8-037a1cc75c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results: [  28. 1025.] [ 2. 10.] [ 27. 300.]\n",
      "PT results: tensor([  28., 1025.], grad_fn=<AddBackward0>) tensor([ 2., 10.]) tensor([ 27., 300.])\n"
     ]
    }
   ],
   "source": [
    "# Brunoflow\n",
    "x_bf = bf.Parameter(np.array([1.0, 5.0]), name=\"x\")\n",
    "y_bf = bf.Parameter(np.array([3.0, 10.0]), name=\"y\")\n",
    "z_bf = x_bf * x_bf + y_bf * y_bf * y_bf\n",
    "z_bf.backprop()\n",
    "print(\"BF results:\", z_bf.val, x_bf.grad, y_bf.grad)\n",
    "\n",
    "# Pytorch\n",
    "x_pt = torch.tensor([1, 5], dtype=torch.float32, requires_grad=True)\n",
    "y_pt = torch.tensor([3, 10], dtype=torch.float32, requires_grad=True)\n",
    "z_pt = x_pt * x_pt + y_pt * y_pt * y_pt\n",
    "\n",
    "# These are the weights to combine the output components dz_1/x and dz_2/x. \n",
    "# (Since the inputs X and Y both are vectors with shape (2,), and output Z is also a vector with shape (2,), \n",
    "#   if you don't combine them then you end up with a (2, 2) gradient output - one for each (output component, input component) pair.\n",
    "#   So how do you combine them? I guess you want to preserve the gradient wrt each input component so you reduce along the axis of the \"output components\".\n",
    "#   See https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments for more info.\n",
    "gradient_weights = torch.FloatTensor([1., 1])\n",
    "z_pt.backward(gradient=gradient_weights) \n",
    "print(\"PT results:\", z_pt, x_pt.grad, y_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aa42a-e2e5-4fcb-8e78-2731158537d4",
   "metadata": {},
   "source": [
    "#### Slightly more complicated array math\n",
    "$$z = x^2 + y^3 + x*y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128d2d9-9b97-4672-9a38-122c88986134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results: [  81. 1078.] [ 8. 30.] [ 29. 310.]\n",
      "PT results: tensor([  81., 1078.], grad_fn=<AddBackward0>) tensor([ 8., 30.]) tensor([ 29., 310.])\n"
     ]
    }
   ],
   "source": [
    "# Brunoflow\n",
    "x_bf = bf.Parameter(np.array([1.0, 5.0]))\n",
    "y_bf = bf.Parameter(np.array([3.0, 10.0]))\n",
    "z_bf = x_bf * x_bf + y_bf * y_bf * y_bf + sum(x_bf * y_bf)\n",
    "z_bf.backprop()\n",
    "print(\"BF results:\", z_bf.val, x_bf.grad, y_bf.grad)\n",
    "\n",
    "# Pytorch\n",
    "x_pt = torch.tensor([1, 5], dtype=torch.float32, requires_grad=True)\n",
    "y_pt = torch.tensor([3, 10], dtype=torch.float32, requires_grad=True)\n",
    "z_pt = x_pt * x_pt + y_pt * y_pt * y_pt + sum(x_pt * y_pt)\n",
    "z_pt.backward(gradient=torch.FloatTensor([1., 1]))\n",
    "print(\"PT results:\", z_pt, x_pt.grad, y_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22629f72-35f0-4a03-b14c-e93923f1b10a",
   "metadata": {},
   "source": [
    "### Linear Layers (with weights and biases all initialized to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784d9ed-5ced-4920-8038-b9460f8a1a7c",
   "metadata": {},
   "source": [
    "#### Brief tangent about using BF - you need to wrap everything in a Node or Parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe62a3-0e45-43c3-843d-cd0fe373d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_bf.grad [[1.]]\n",
      "computing input_bf.grad fails! with the following error: 'numpy.ndarray' object has no attribute 'grad'\n",
      "output_bf.grad [[1.]]\n",
      "input_bf.grad [[1. 1. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/interpreta-bauer-ly/brunoflow/func/linalg.py:256: RuntimeWarning: divide by zero encountered in log\n",
      "  + np.matmul(__np_matrix_transpose(-A_factor * np.log(A_factor)), out_abs_val_grad),\n",
      "/home/kevin/code/rycolab/interpreta-bauer-ly/brunoflow/func/linalg.py:256: RuntimeWarning: invalid value encountered in multiply\n",
      "  + np.matmul(__np_matrix_transpose(-A_factor * np.log(A_factor)), out_abs_val_grad),\n"
     ]
    }
   ],
   "source": [
    "# Brunoflow WITHOUT np arrays wrapped in nodes\n",
    "ff1_bf = bf.net.LinearInitToOne(3, 1)\n",
    "input_bf = np.expand_dims(np.array(range(0, 3)), axis=0)\n",
    "output_bf = ff1_bf(input_bf)\n",
    "output_bf.backprop()\n",
    "print(\"output_bf.grad\", output_bf.grad)\n",
    "try:\n",
    "    print(\"input_bf.grad\", input_bf.grad)\n",
    "except AttributeError as e:\n",
    "    print(f\"computing input_bf.grad fails! with the following error: {e}\")\n",
    "    \n",
    "assert(isinstance(output_bf.inputs[0].inputs[0], np.ndarray))\n",
    "\n",
    "# Brunoflow WITH np arrays wrapped in nodes\n",
    "ff1_bf = bf.net.LinearInitToOne(3, 1)\n",
    "input_bf = Node(np.expand_dims(np.array(range(0, 3)), axis=0))\n",
    "output_bf = ff1_bf(input_bf)\n",
    "output_bf.backprop()\n",
    "print(\"output_bf.grad\", output_bf.grad)\n",
    "print(\"input_bf.grad\", input_bf.grad)\n",
    "assert(isinstance(output_bf.inputs[0].inputs[0], Node))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f20342-795a-4a33-95cf-c95688104d7d",
   "metadata": {},
   "source": [
    "#### Ok, now actually comparing BF to Pytorch for a linear network with a single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9875d0-cfd7-498a-b4bf-69225a0778da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results:\n",
      "output_bf.val [[4.]]\n",
      "output_bf.grad [[1.]]\n",
      "input_bf.grad [[1. 1. 1.]]\n",
      "\n",
      "Pytorch results:\n",
      "output_pt value tensor([[4.]], grad_fn=<AddmmBackward0>)\n",
      "output_pt.grad tensor([[1.]])\n",
      "input_pt.grad tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Brunoflow\n",
    "ff1_bf = bf.net.LinearInitToOne(3, 1)\n",
    "input_bf = Node(np.expand_dims(np.array(range(0, 3)), axis=0))\n",
    "output_bf = ff1_bf(input_bf)\n",
    "output_bf.backprop()\n",
    "print(\"BF results:\")\n",
    "print(\"output_bf.val\", output_bf.val)\n",
    "print(\"output_bf.grad\", output_bf.grad)\n",
    "print(\"input_bf.grad\", input_bf.grad)\n",
    "print()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 1)\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.ones_like(ff1_pt.weight))\n",
    "    ff1_pt.bias = nn.Parameter(torch.ones_like(ff1_pt.bias))\n",
    "    \n",
    "input_pt = torch.tensor(np.expand_dims(np.array(range(0, 3)), axis=0), dtype=torch.float32, requires_grad=True)\n",
    "output_pt = ff1_pt(input_pt)\n",
    "\n",
    "input_pt.is_leaf # True, because we created it\n",
    "output_pt.is_leaf # False, because it's computed by some operations involving other tensors\n",
    "\n",
    "# Since output_pt is not a leaf node (e.g. not created by the user and requires_grad = True), \n",
    "# we need to explicitly tell pytorch to compute the gradient w.r.t. to this variable\n",
    "output_pt.retain_grad()\n",
    "output_pt.backward()\n",
    "print(\"Pytorch results:\")\n",
    "print(\"output_pt value\", output_pt)\n",
    "print(\"output_pt.grad\", output_pt.grad)\n",
    "print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950674e3-3cb1-4805-afed-2ddbe18bb2a9",
   "metadata": {},
   "source": [
    "#### BF vs PYT for linear network with multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bd4ce-305d-4882-8fcd-ca66f9fb232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results:\n",
      "output_bf.val [[4. 4.]]\n",
      "output_bf.grad [[1. 1.]]\n",
      "input_bf.grad [[2. 2. 2.]]\n",
      "\n",
      "Pytorch results:\n",
      "output_pt value tensor([[4., 4.]], grad_fn=<AddmmBackward0>)\n",
      "output_pt.grad tensor([[1., 1.]])\n",
      "input_pt.grad tensor([[2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Brunoflow\n",
    "ff1_bf = bf.net.LinearInitToOne(3, 2)\n",
    "input_bf = Node(np.array([range(0, 3)])) # shape (1, 3)\n",
    "output_bf = ff1_bf(input_bf)\n",
    "output_bf.backprop()\n",
    "print(\"BF results:\")\n",
    "print(\"output_bf.val\", output_bf.val)\n",
    "print(\"output_bf.grad\", output_bf.grad)\n",
    "print(\"input_bf.grad\", input_bf.grad)\n",
    "print()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 2)\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.ones_like(ff1_pt.weight))\n",
    "    ff1_pt.bias = nn.Parameter(torch.ones_like(ff1_pt.bias))\n",
    "    \n",
    "input_pt = torch.tensor(np.array([range(0, 3)]), dtype=torch.float32, requires_grad=True)\n",
    "output_pt = ff1_pt(input_pt) # shape (1, 2)\n",
    "output_pt.retain_grad()\n",
    "\n",
    "# Evidently, the \"gradient\" parameter here needs to be of shape (1, 2) to match the shape of the *output* layer.\n",
    "# This means that for each output value there's a gradient being computed...somehow? And the weight of each input is 1. in this case.\n",
    "# Well, the takeaway here appears to be to simply use a tensor of 1s of the same shape as the output layer and we'll get the same value as for BF.\n",
    "output_pt.backward(gradient=torch.FloatTensor([[1., 1.]]))\n",
    "print(\"Pytorch results:\")\n",
    "print(\"output_pt value\", output_pt)\n",
    "print(\"output_pt.grad\", output_pt.grad)\n",
    "print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce75c02-bcfc-4813-9a36-a10b55bdbcf3",
   "metadata": {},
   "source": [
    "#### BF vs PYT for linear network with multiple inputs and outputs (input shape (2, 3) -> output shape (2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d52ba5-f316-4e7e-a8c6-f87ef8e31d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results:\n",
      "output_bf.val [[ 4.  4.  4.  4.]\n",
      " [16. 16. 16. 16.]]\n",
      "output_bf.grad [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "input_bf.grad [[4. 4. 4.]\n",
      " [4. 4. 4.]]\n",
      "\n",
      "Pytorch results:\n",
      "output_pt value tensor([[ 4.,  4.,  4.,  4.],\n",
      "        [16., 16., 16., 16.]], grad_fn=<AddmmBackward0>)\n",
      "output_pt.grad tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "input_pt.grad tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Brunoflow\n",
    "ff1_bf = bf.net.LinearInitToOne(3, 4)\n",
    "input_bf = Node(np.array([range(0, 3), range(4,7)]))\n",
    "output_bf = ff1_bf(input_bf)\n",
    "output_bf.backprop()\n",
    "print(\"BF results:\")\n",
    "print(\"output_bf.val\", output_bf.val)\n",
    "print(\"output_bf.grad\", output_bf.grad)\n",
    "print(\"input_bf.grad\", input_bf.grad)\n",
    "print()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 4)\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.ones_like(ff1_pt.weight))\n",
    "    ff1_pt.bias = nn.Parameter(torch.ones_like(ff1_pt.bias))\n",
    "    \n",
    "input_pt = torch.tensor(np.array([range(0, 3), range(4,7)]), dtype=torch.float32, requires_grad=True)\n",
    "output_pt = ff1_pt(input_pt)\n",
    "output_pt.retain_grad()\n",
    "output_pt.backward(gradient=torch.FloatTensor([[1., 1., 1., 1.], [1., 1., 1., 1.]]))\n",
    "print(\"Pytorch results:\")\n",
    "print(\"output_pt value\", output_pt)\n",
    "print(\"output_pt.grad\", output_pt.grad)\n",
    "print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b051b2ad-edec-4394-ac6a-4516adb6fbff",
   "metadata": {},
   "source": [
    "### Linear Layer with activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05e846-c61c-4e72-9d1f-d384252544f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node(name: (exp (- [1 1 1] (expand_dims (+ (log (sum (exp (- [1 1 1] (expand_dims (max [1 1 1] axis=0) axis=0))) axis=0)) (max [1 1 1] axis=0)) axis=0))), val: [0.33333333 0.33333333 0.33333333], grad: [0. 0. 0.])\n",
      "node(name: (- [1 1 1] (expand_dims (+ (log (sum (exp (- [1 1 1] (expand_dims (max [1 1 1] axis=0) axis=0))) axis=0)) (max [1 1 1] axis=0)) axis=0)), val: [-1.09861229 -1.09861229 -1.09861229], grad: [0. 0. 0.])\n",
      "node(name: (max [ 1 -1  1] 0), val: [1 0 1], grad: [0. 0. 0.])\n"
     ]
    }
   ],
   "source": [
    "from brunoflow.func import softmax, log_softmax, relu\n",
    "print(softmax(np.array([1, 1, 1]), axis=0))\n",
    "print(log_softmax(np.array([1, 1, 1]), axis=0))\n",
    "print(relu(np.array([1, -1, 1])))\n",
    "\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e38ea9-2efb-48c6-b12b-254d5e3390e9",
   "metadata": {},
   "source": [
    "#### Softmax - this actually faces some numerical stability errors so you won't see them exactly match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b24321-873e-46f1-aacc-b234bd0ea52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results:\n",
      "softmax_bf.val [[0.0016588 0.9983412]]\n",
      "softmax_bf.grad [[1. 1.]]\n",
      "ff1_output_bf.val [[3.  9.4]]\n",
      "ff1_output_bf.grad [[-4.33680869e-19  0.00000000e+00]]\n",
      "input_bf.val [[1. 2. 1.]]\n",
      "input_bf.grad [[-2.16840434e-19 -2.16840434e-19 -2.16840434e-19]]\n",
      "\n",
      "Pytorch results:\n",
      "softmax_pt value tensor([[0.00165880, 0.99834120]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "softmax_pt.grad tensor([[1., 1.]], dtype=torch.float64)\n",
      "ff1_output_pt value tensor([[3.00000000, 9.40000000]], dtype=torch.float64,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ff1_output_pt.grad tensor([[0., 0.]], dtype=torch.float64)\n",
      "input_pt value tensor([[1., 2., 1.]], dtype=torch.float64, requires_grad=True)\n",
      "input_pt.grad tensor([[0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Shared\n",
    "# input size = 3, output size = 2\n",
    "output_layer_weights = np.array([0.5, 2.1], dtype=np.float64)\n",
    "W = np.tile(output_layer_weights, (3, 1)) # to use tile, you give it the number of repetitions you want in each axis\n",
    "b = np.ones(shape=(2,), dtype=np.float64)\n",
    "# input_arr = np.array([range(0, 3)])\n",
    "input_arr = np.array([[1.,2.,1.]], dtype=np.float64)\n",
    "\n",
    "# Brunoflow\n",
    "ff1_bf = bf.net.Linear(3, 2)\n",
    "ff1_bf.set_weights(W)\n",
    "ff1_bf.set_bias(b)\n",
    "\n",
    "input_bf = Node(input_arr) # shape (1, 3)\n",
    "ff1_output_bf = ff1_bf(input_bf)\n",
    "softmax_bf = softmax(ff1_output_bf, axis=1)\n",
    "softmax_bf.backprop()\n",
    "print(\"BF results:\")\n",
    "print(\"softmax_bf.val\", softmax_bf.val)\n",
    "print(\"softmax_bf.grad\", softmax_bf.grad)\n",
    "print(\"ff1_output_bf.val\", ff1_output_bf.val)\n",
    "print(\"ff1_output_bf.grad\", ff1_output_bf.grad)\n",
    "print(\"input_bf.val\", input_bf.val)\n",
    "print(\"input_bf.grad\", input_bf.grad)\n",
    "print()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 2)\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.tensor(W.transpose(), dtype=torch.float64))\n",
    "    ff1_pt.bias = nn.Parameter(torch.tensor(b, dtype=torch.float64))\n",
    "    \n",
    "input_pt = torch.tensor(input_arr, dtype=torch.float64, requires_grad=True)\n",
    "ff1_output_pt = ff1_pt(input_pt) # shape (1, 2)\n",
    "ff1_output_pt.retain_grad()\n",
    "softmax_pt = nn.functional.softmax(ff1_output_pt, dim=1)\n",
    "softmax_pt.retain_grad()\n",
    "\n",
    "# Evidently, the \"gradient\" parameter here needs to be of shape (1, 2) to match the shape of the *output* layer.\n",
    "# This means that for each output value there's a gradient being computed...somehow? And the weight of each input is 1. in this case.\n",
    "# Well, the takeaway here appears to be to simply use a tensor of 1s of the same shape as the output layer and we'll get the same value as for BF.\n",
    "softmax_pt.backward(gradient=torch.tensor([[1., 1.]], dtype=torch.float64))\n",
    "\n",
    "print(\"Pytorch results:\")\n",
    "print(\"softmax_pt value\", softmax_pt)\n",
    "print(\"softmax_pt.grad\", softmax_pt.grad)\n",
    "print(\"ff1_output_pt value\", ff1_output_pt)\n",
    "print(\"ff1_output_pt.grad\", ff1_output_pt.grad)\n",
    "print(\"input_pt value\", input_pt)\n",
    "print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f2da8-ee31-44ba-817b-55973c86ac13",
   "metadata": {},
   "source": [
    "#### Log softmax - luckily, this is not nearly as unstable and you see the results match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaefbd9-81fa-4d29-bb12-037ffea19e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results:\n",
      "softmax_bf.val [[-7.20074631e+00 -7.46307252e-04]]\n",
      "softmax_bf.grad [[1. 1.]]\n",
      "ff1_output_bf.val [[ 3.  10.2]]\n",
      "ff1_output_bf.grad [[ 0.99850794 -0.99850794]]\n",
      "input_bf.val [[1. 2. 1.]]\n",
      "input_bf.grad [[-1.7973143 -1.7973143 -1.7973143]]\n",
      "\n",
      "Pytorch results:\n",
      "softmax_pt value tensor([[-7.20074631e+00, -7.46307252e-04]], dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "softmax_pt.grad tensor([[1., 1.]], dtype=torch.float64)\n",
      "ff1_output_pt value tensor([[ 3.00000000, 10.20000000]], dtype=torch.float64,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ff1_output_pt.grad tensor([[ 0.99850794, -0.99850794]], dtype=torch.float64)\n",
      "input_pt value tensor([[1., 2., 1.]], dtype=torch.float64, requires_grad=True)\n",
      "input_pt.grad tensor([[-1.79731430, -1.79731430, -1.79731430]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Shared\n",
    "# input size = 3, output size = 2\n",
    "output_layer_weights = np.array([0.5, 2.3], dtype=np.float64)\n",
    "W = np.tile(output_layer_weights, (3, 1)) # to use tile, you give it the number of repetitions you want in each axis\n",
    "b = np.ones(shape=(2,), dtype=np.float64)\n",
    "# input_arr = np.array([range(0, 3)])\n",
    "input_arr = np.array([[1.,2.,1.]], dtype=np.float64)\n",
    "\n",
    "# Brunoflow\n",
    "ff1_bf = bf.net.Linear(3, 2)\n",
    "ff1_bf.set_weights(W)\n",
    "ff1_bf.set_bias(b)\n",
    "\n",
    "input_bf = Node(input_arr) # shape (1, 3)\n",
    "ff1_output_bf = ff1_bf(input_bf)\n",
    "softmax_bf = log_softmax(ff1_output_bf, axis=1)\n",
    "softmax_bf.backprop()\n",
    "print(\"BF results:\")\n",
    "print(\"softmax_bf.val\", softmax_bf.val)\n",
    "print(\"softmax_bf.grad\", softmax_bf.grad)\n",
    "print(\"ff1_output_bf.val\", ff1_output_bf.val)\n",
    "print(\"ff1_output_bf.grad\", ff1_output_bf.grad)\n",
    "print(\"input_bf.val\", input_bf.val)\n",
    "print(\"input_bf.grad\", input_bf.grad)\n",
    "print()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 2)\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.tensor(W.transpose(), dtype=torch.float64))\n",
    "    ff1_pt.bias = nn.Parameter(torch.tensor(b, dtype=torch.float64))\n",
    "    \n",
    "input_pt = torch.tensor(input_arr, dtype=torch.float64, requires_grad=True)\n",
    "ff1_output_pt = ff1_pt(input_pt) # shape (1, 2)\n",
    "ff1_output_pt.retain_grad()\n",
    "softmax_pt = nn.functional.log_softmax(ff1_output_pt, dim=1)\n",
    "softmax_pt.retain_grad()\n",
    "\n",
    "# Evidently, the \"gradient\" parameter here needs to be of shape (1, 2) to match the shape of the *output* layer.\n",
    "# This means that for each output value there's a gradient being computed...somehow? And the weight of each input is 1. in this case.\n",
    "# Well, the takeaway here appears to be to simply use a tensor of 1s of the same shape as the output layer and we'll get the same value as for BF.\n",
    "softmax_pt.backward(gradient=torch.tensor([[1., 1.]], dtype=torch.float64))\n",
    "\n",
    "print(\"Pytorch results:\")\n",
    "print(\"softmax_pt value\", softmax_pt)\n",
    "print(\"softmax_pt.grad\", softmax_pt.grad)\n",
    "print(\"ff1_output_pt value\", ff1_output_pt)\n",
    "print(\"ff1_output_pt.grad\", ff1_output_pt.grad)\n",
    "print(\"input_pt value\", input_pt)\n",
    "print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c92fd-3560-41dd-82dd-3dc7dcc3017c",
   "metadata": {},
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263b98e-9529-49c7-9ba2-8d5871f70506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF results:\n",
      "relu_bf.val [[ 3.  10.2]]\n",
      "relu_bf.grad [[1. 1.]]\n",
      "ff1_output_bf.val [[ 3.  10.2]]\n",
      "ff1_output_bf.grad [[1. 1.]]\n",
      "input_bf.val [[1. 2. 1.]]\n",
      "input_bf.grad [[2.8 2.8 2.8]]\n",
      "\n",
      "Pytorch results:\n",
      "relu_pt value tensor([[ 3.00000000, 10.20000000]], dtype=torch.float64,\n",
      "       grad_fn=<ReluBackward0>)\n",
      "relu_pt.grad tensor([[1., 1.]], dtype=torch.float64)\n",
      "ff1_output_pt value tensor([[ 3.00000000, 10.20000000]], dtype=torch.float64,\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "ff1_output_pt.grad tensor([[1., 1.]], dtype=torch.float64)\n",
      "input_pt value tensor([[1., 2., 1.]], dtype=torch.float64, requires_grad=True)\n",
      "input_pt.grad tensor([[2.80000000, 2.80000000, 2.80000000]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/code/rycolab/interpreta-bauer-ly/brunoflow/ad/node.py:277: RuntimeWarning: divide by zero encountered in log\n",
      "  + (-np.abs(l_adj) * np.log(np.abs(l_adj)) * out_grad_and_entropy_dict[\"out_abs_val_grad\"])\n",
      "/home/kevin/code/rycolab/interpreta-bauer-ly/brunoflow/ad/node.py:277: RuntimeWarning: invalid value encountered in multiply\n",
      "  + (-np.abs(l_adj) * np.log(np.abs(l_adj)) * out_grad_and_entropy_dict[\"out_abs_val_grad\"])\n"
     ]
    }
   ],
   "source": [
    "# Shared\n",
    "# input size = 3, output size = 2\n",
    "output_layer_weights = np.array([0.5, 2.3], dtype=np.float64)\n",
    "W = np.tile(output_layer_weights, (3, 1)) # to use tile, you give it the number of repetitions you want in each axis\n",
    "b = np.ones(shape=(2,), dtype=np.float64)\n",
    "# input_arr = np.array([range(0, 3)])\n",
    "input_arr = np.array([[1.,2.,1.]], dtype=np.float64)\n",
    "\n",
    "# Brunoflow\n",
    "ff1_bf = bf.net.Linear(3, 2)\n",
    "ff1_bf.set_weights(W)\n",
    "ff1_bf.set_bias(b)\n",
    "\n",
    "input_bf = Node(input_arr) # shape (1, 3)\n",
    "ff1_output_bf = ff1_bf(input_bf)\n",
    "relu_bf = relu(ff1_output_bf)\n",
    "relu_bf.backprop()\n",
    "print(\"BF results:\")\n",
    "print(\"relu_bf.val\", relu_bf.val)\n",
    "print(\"relu_bf.grad\", relu_bf.grad)\n",
    "print(\"ff1_output_bf.val\", ff1_output_bf.val)\n",
    "print(\"ff1_output_bf.grad\", ff1_output_bf.grad)\n",
    "print(\"input_bf.val\", input_bf.val)\n",
    "print(\"input_bf.grad\", input_bf.grad)\n",
    "print()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 2)\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.tensor(W.transpose(), dtype=torch.float64))\n",
    "    ff1_pt.bias = nn.Parameter(torch.tensor(b, dtype=torch.float64))\n",
    "    \n",
    "input_pt = torch.tensor(input_arr, dtype=torch.float64, requires_grad=True)\n",
    "ff1_output_pt = ff1_pt(input_pt) # shape (1, 2)\n",
    "ff1_output_pt.retain_grad()\n",
    "relu_pt = nn.functional.relu(ff1_output_pt)\n",
    "relu_pt.retain_grad()\n",
    "\n",
    "# Evidently, the \"gradient\" parameter here needs to be of shape (1, 2) to match the shape of the *output* layer.\n",
    "# This means that for each output value there's a gradient being computed...somehow? And the weight of each input is 1. in this case.\n",
    "# Well, the takeaway here appears to be to simply use a tensor of 1s of the same shape as the output layer and we'll get the same value as for BF.\n",
    "relu_pt.backward(gradient=torch.tensor([[1., 1.]], dtype=torch.float64))\n",
    "\n",
    "print(\"Pytorch results:\")\n",
    "print(\"relu_pt value\", relu_pt)\n",
    "print(\"relu_pt.grad\", relu_pt.grad)\n",
    "print(\"ff1_output_pt value\", ff1_output_pt)\n",
    "print(\"ff1_output_pt.grad\", ff1_output_pt.grad)\n",
    "print(\"input_pt value\", input_pt)\n",
    "print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db68ae7-62c6-4e09-93aa-4f1acf11b18f",
   "metadata": {},
   "source": [
    "### How about with some loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a9a403-cf15-40f1-87d8-4390cedec0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brunoflow.opt import cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131729f-4436-4d52-b8b3-6916d237c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers have close enough values between BF and pytorch!\n"
     ]
    }
   ],
   "source": [
    "# Shared\n",
    "# input size = 3, output size = 8 (but actually a scalar because we're looking at loss here)\n",
    "input_arr = np.array([[1.,2.,1.]], dtype=np.float64)\n",
    "target = np.array([0])\n",
    "\n",
    "# Brunoflow\n",
    "ff1_bf = bf.net.Linear(3, 8)\n",
    "W_ff1 = ff1_bf.W.val\n",
    "b_ff1 = ff1_bf.b.val\n",
    "\n",
    "input_bf = Node(input_arr) # shape (1, 3)\n",
    "ff1_output_bf = ff1_bf(input_bf)\n",
    "loss_bf = cross_entropy_loss(ff1_output_bf, target=target)\n",
    "loss_bf.backprop()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.tensor(W_ff1.transpose(), dtype=torch.float64))\n",
    "    ff1_pt.bias = nn.Parameter(torch.tensor(b_ff1, dtype=torch.float64))\n",
    "    \n",
    "input_pt = torch.tensor(input_arr, dtype=torch.float64, requires_grad=True)\n",
    "ff1_output_pt = ff1_pt(input_pt) # shape (1, 2)\n",
    "ff1_output_pt.retain_grad()\n",
    "loss_pt = nn.functional.cross_entropy(ff1_output_pt, target=torch.tensor(target))\n",
    "loss_pt.retain_grad()\n",
    "\n",
    "# Since the gradient here is w.r.t. a scalar (the loss), we don't really need to fill in the gradient function, \n",
    "# but to be super explicit here it is as just...the scalar 1!\n",
    "loss_pt.backward(gradient=torch.tensor(1.))\n",
    "\n",
    "# Check near-equality\n",
    "try:\n",
    "    assert(torch.allclose(loss_pt, torch.tensor(loss_bf.val)))\n",
    "    assert(torch.allclose(ff1_output_pt, torch.tensor(ff1_output_bf.val)))\n",
    "    assert(torch.allclose(input_pt, torch.tensor(input_bf.val)))\n",
    "    print(\"All layers have close enough values between BF and pytorch!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Uhoh! torch and bf had a difference with following error - {e}\")\n",
    "    \n",
    "    print(\"BF results:\")\n",
    "    print(\"loss_bf\", loss_bf)\n",
    "    print(\"ff1_output_bf\", ff1_output_bf)\n",
    "    print(\"input_bf\", input_bf)\n",
    "    print()\n",
    "\n",
    "    print(\"Pytorch results:\")\n",
    "    print(\"loss_pt value\", loss_pt)\n",
    "    print(\"loss_pt.grad\", loss_pt.grad)\n",
    "    print(\"ff1_output_pt value\", ff1_output_pt)\n",
    "    print(\"ff1_output_pt.grad\", ff1_output_pt.grad)\n",
    "    print(\"input_pt value\", input_pt)\n",
    "    print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd5c16-2249-4eb1-a57c-ba5a88ff5afb",
   "metadata": {},
   "source": [
    "### Time for some MLPs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519e112-431a-4f05-977b-b9bd3875260f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers have close enough values between BF and pytorch!\n"
     ]
    }
   ],
   "source": [
    "# Shared\n",
    "# input size = 3, output size = 2\n",
    "input_arr = np.array([[1.,2.,1.]], dtype=np.float64)\n",
    "\n",
    "# Brunoflow\n",
    "ff1_bf = bf.net.Linear(3, 10)\n",
    "ff2_bf = bf.net.Linear(10, 2)\n",
    "W_ff1 = ff1_bf.W.val\n",
    "b_ff1 = ff1_bf.b.val\n",
    "W_ff2 = ff2_bf.W.val\n",
    "b_ff2 = ff2_bf.b.val\n",
    "\n",
    "input_bf = Node(input_arr) # shape (1, 3)\n",
    "ff1_output_bf = ff1_bf(input_bf)\n",
    "relu_bf = relu(ff1_output_bf)\n",
    "ff2_output_bf = ff2_bf(relu_bf)\n",
    "log_softmax_bf = log_softmax(ff2_output_bf, axis=1)\n",
    "log_softmax_bf.backprop()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 10)\n",
    "ff2_pt = nn.Linear(10, 2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.tensor(W_ff1.transpose(), dtype=torch.float64))\n",
    "    ff1_pt.bias = nn.Parameter(torch.tensor(b_ff1, dtype=torch.float64))\n",
    "    ff2_pt.weight = nn.Parameter(torch.tensor(W_ff2.transpose(), dtype=torch.float64))\n",
    "    ff2_pt.bias = nn.Parameter(torch.tensor(b_ff2, dtype=torch.float64))\n",
    "    \n",
    "input_pt = torch.tensor(input_arr, dtype=torch.float64, requires_grad=True)\n",
    "ff1_output_pt = ff1_pt(input_pt) # shape (1, 2)\n",
    "ff1_output_pt.retain_grad()\n",
    "relu_pt = nn.functional.relu(ff1_output_pt)\n",
    "relu_pt.retain_grad()\n",
    "ff2_output_pt = ff2_pt(relu_pt)\n",
    "ff2_output_pt.retain_grad()\n",
    "log_softmax_pt = nn.functional.log_softmax(ff2_output_pt, dim=1)\n",
    "log_softmax_pt.retain_grad()\n",
    "\n",
    "# # Evidently, the \"gradient\" parameter here needs to be of shape (1, 2) to match the shape of the *output* layer.\n",
    "# # This means that for each output value there's a gradient being computed...somehow? And the weight of each input is 1. in this case.\n",
    "# # Well, the takeaway here appears to be to simply use a tensor of 1s of the same shape as the output layer and we'll get the same value as for BF.\n",
    "log_softmax_pt.backward(gradient=torch.tensor([[1., 1.]], dtype=torch.float64))\n",
    "\n",
    "# Check near-equality\n",
    "try:\n",
    "    assert(torch.allclose(log_softmax_pt, torch.tensor(log_softmax_bf.val)))\n",
    "    assert(torch.allclose(ff2_output_pt, torch.tensor(ff2_output_bf.val)))\n",
    "    assert(torch.allclose(relu_pt, torch.tensor(relu_bf.val)))\n",
    "    assert(torch.allclose(ff1_output_pt, torch.tensor(ff1_output_bf.val)))\n",
    "    assert(torch.allclose(input_pt, torch.tensor(input_bf.val)))\n",
    "    print(\"All layers have close enough values between BF and pytorch!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Uhoh! torch and bf had a difference with following error - {e}\")\n",
    "    \n",
    "    print(\"BF results:\")\n",
    "    print(\"log_softmax_bf\", log_softmax_bf)\n",
    "    print(\"ff2_output_bf\", ff2_output_bf)\n",
    "    print(\"relu_bf\", relu_bf)\n",
    "    print(\"ff1_output_bf\", ff1_output_bf)\n",
    "    print(\"input_bf\", input_bf)\n",
    "    print()\n",
    "\n",
    "    print(\"Pytorch results:\")\n",
    "    print(\"log_softmax_pt value\", log_softmax_pt)\n",
    "    print(\"log_softmax_pt.grad\", log_softmax_pt.grad)\n",
    "    print(\"ff2_output_pt value\", ff2_output_pt)\n",
    "    print(\"ff2_output_pt.grad\", ff2_output_pt.grad)\n",
    "    print(\"relu_pt value\", relu_pt)\n",
    "    print(\"relu_pt.grad\", relu_pt.grad)\n",
    "    print(\"ff1_output_pt value\", ff1_output_pt)\n",
    "    print(\"ff1_output_pt.grad\", ff1_output_pt.grad)\n",
    "    print(\"input_pt value\", input_pt)\n",
    "    print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e6ed8-8b06-4b58-92cd-c576147d2dbd",
   "metadata": {},
   "source": [
    "#### MLP with multiple inputs and loss! (putting it mostly all together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f801fd55-1966-496c-9385-55f075e0959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers have close enough values between BF and pytorch!\n"
     ]
    }
   ],
   "source": [
    "# Shared\n",
    "# input size = 3, output size = 2, batch_size = 2\n",
    "input_arr = np.array([[1.,2.,1.], [2,4,6]], dtype=np.float64)\n",
    "target = np.array([0, 1])\n",
    "\n",
    "# Brunoflow\n",
    "ff1_bf = bf.net.Linear(3, 10)\n",
    "ff2_bf = bf.net.Linear(10, 2)\n",
    "W_ff1 = ff1_bf.W.val\n",
    "b_ff1 = ff1_bf.b.val\n",
    "W_ff2 = ff2_bf.W.val\n",
    "b_ff2 = ff2_bf.b.val\n",
    "\n",
    "input_bf = Node(input_arr) # shape (1, 3)\n",
    "ff1_output_bf = ff1_bf(input_bf)\n",
    "relu_bf = relu(ff1_output_bf)\n",
    "ff2_output_bf = ff2_bf(relu_bf)\n",
    "loss_bf = cross_entropy_loss(ff2_output_bf, target=target)\n",
    "loss_bf.backprop()\n",
    "\n",
    "# Pytorch\n",
    "ff1_pt = nn.Linear(3, 10)\n",
    "ff2_pt = nn.Linear(10, 2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ff1_pt.weight = nn.Parameter(torch.tensor(W_ff1.transpose(), dtype=torch.float64))\n",
    "    ff1_pt.bias = nn.Parameter(torch.tensor(b_ff1, dtype=torch.float64))\n",
    "    ff2_pt.weight = nn.Parameter(torch.tensor(W_ff2.transpose(), dtype=torch.float64))\n",
    "    ff2_pt.bias = nn.Parameter(torch.tensor(b_ff2, dtype=torch.float64))\n",
    "    \n",
    "input_pt = torch.tensor(input_arr, dtype=torch.float64, requires_grad=True)\n",
    "ff1_output_pt = ff1_pt(input_pt) # shape (1, 2)\n",
    "ff1_output_pt.retain_grad()\n",
    "relu_pt = nn.functional.relu(ff1_output_pt)\n",
    "relu_pt.retain_grad()\n",
    "ff2_output_pt = ff2_pt(relu_pt)\n",
    "ff2_output_pt.retain_grad()\n",
    "loss_pt = nn.functional.cross_entropy(ff2_output_pt, target=torch.tensor(target))\n",
    "loss_pt.retain_grad()\n",
    "\n",
    "# Since the gradient here is w.r.t. a scalar (the loss), we don't really need to fill in the gradient function, \n",
    "# but to be super explicit here it is as just...the scalar 1!\n",
    "loss_pt.backward(gradient=torch.tensor(1.))\n",
    "\n",
    "# Check near-equality\n",
    "try:\n",
    "    assert(torch.allclose(loss_pt, torch.tensor(loss_bf.val)))\n",
    "    assert(torch.allclose(ff2_output_pt, torch.tensor(ff2_output_bf.val)))\n",
    "    assert(torch.allclose(relu_pt, torch.tensor(relu_bf.val)))\n",
    "    assert(torch.allclose(ff1_output_pt, torch.tensor(ff1_output_bf.val)))\n",
    "    assert(torch.allclose(input_pt, torch.tensor(input_bf.val)))\n",
    "    print(\"All layers have close enough values between BF and pytorch!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Uhoh! torch and bf had a difference with following error - {e}\")\n",
    "    \n",
    "    print(\"BF results:\")\n",
    "    print(\"loss_bf\", loss_bf)\n",
    "    print(\"ff2_output_bf\", ff2_output_bf)\n",
    "    print(\"relu_bf\", relu_bf)\n",
    "    print(\"ff1_output_bf\", ff1_output_bf)\n",
    "    print(\"input_bf\", input_bf)\n",
    "    print()\n",
    "\n",
    "    print(\"Pytorch results:\")\n",
    "    print(\"loss_pt value\", loss_pt)\n",
    "    print(\"loss_pt.grad\", loss_pt.grad)\n",
    "    print(\"ff2_output_pt value\", ff2_output_pt)\n",
    "    print(\"ff2_output_pt.grad\", ff2_output_pt.grad)\n",
    "    print(\"relu_pt value\", relu_pt)\n",
    "    print(\"relu_pt.grad\", relu_pt.grad)\n",
    "    print(\"ff1_output_pt value\", ff1_output_pt)\n",
    "    print(\"ff1_output_pt.grad\", ff1_output_pt.grad)\n",
    "    print(\"input_pt value\", input_pt)\n",
    "    print(\"input_pt.grad\", input_pt.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a1aa9-86ed-4bf7-a571-f1e2d9556470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig!\n"
     ]
    }
   ],
   "source": [
    "print(\"Fertig!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71efda3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch grad before backward: None\n",
      "x torch grad after backward: tensor([[-0.0931,  0.0611,  0.5228]])\n",
      "x bf grad before backward: [[0. 0. 0.]]\n",
      "(1, 10)\n",
      "x bf grad after backward: [[-0.09305926  0.0610918   0.5227769 ]]\n"
     ]
    }
   ],
   "source": [
    "from jax import numpy as jnp\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.tensor([[1., 2., 3.]], requires_grad=True)\n",
    "target_class = 8\n",
    "linear_torch = torch.nn.Linear(3, 10)\n",
    "out = linear_torch(x)\n",
    "print(\"x torch grad before backward:\", x.grad)\n",
    "out[:, target_class].backward()\n",
    "print(\"x torch grad after backward:\",x.grad)\n",
    "\n",
    "x_bf = bf.Node(jnp.array(x.detach().numpy()), name=\"x_bf\")\n",
    "linear_bf = bf.net.Linear(3, 10)\n",
    "linear_bf.weight.val = jnp.array(linear_torch.weight.detach().numpy())\n",
    "linear_bf.bias.val = jnp.array(linear_torch.bias.detach().numpy())\n",
    "out_bf = linear_bf(x_bf)\n",
    "print(\"x bf grad before backward:\", x_bf.grad)\n",
    "out_bf[0, target_class].backprop()\n",
    "print(out_bf.shape)\n",
    "print(\"x bf grad after backward:\", x_bf.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4c9e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch grad before backward: None\n",
      "x torch grad after backward: tensor([[-1.3587, -0.4561,  0.6417]])\n",
      "x bf grad before backward: [[0. 0. 0.]]\n",
      "(1, 10)\n",
      "x bf grad after backward: [[-1.3587005 -0.4560883  0.6416915]]\n"
     ]
    }
   ],
   "source": [
    "from jax import numpy as jnp\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.tensor([[1., 2., 3.]], requires_grad=True)\n",
    "target_class = 8\n",
    "linear_torch = torch.nn.Linear(3, 10)\n",
    "out = linear_torch(x)\n",
    "print(\"x torch grad before backward:\", x.grad)\n",
    "out.backward(gradient=torch.ones_like(out))\n",
    "print(\"x torch grad after backward:\",x.grad)\n",
    "\n",
    "x_bf = bf.Node(jnp.array(x.detach().numpy()), name=\"x_bf\")\n",
    "linear_bf = bf.net.Linear(3, 10)\n",
    "linear_bf.weight.val = jnp.array(linear_torch.weight.detach().numpy())\n",
    "linear_bf.bias.val = jnp.array(linear_torch.bias.detach().numpy())\n",
    "out_bf = linear_bf(x_bf)\n",
    "print(\"x bf grad before backward:\", x_bf.grad)\n",
    "out_bf.backprop()\n",
    "print(out_bf.shape)\n",
    "print(\"x bf grad after backward:\", x_bf.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781c6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
