{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import brunoflow as bf\n",
    "from brunoflow.ad.utils import check_node_equals_tensor\n",
    "from jax import numpy as jnp, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 16:01:18.264866: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "random_key_val = 42\n",
    "num_embeddings = 5\n",
    "embedding_dim = 3\n",
    "padding_idx = 1\n",
    "random_key = random.PRNGKey(random_key_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0813,  1.1262,  0.3836],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [-1.1691,  0.5396,  0.3151],\n",
       "        [-0.1995, -0.8717,  1.5982],\n",
       "        [-0.1942, -0.6943, -0.2199]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and inspect a torch embedding\n",
    "import torch\n",
    "emb = torch.nn.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [4. 4. 4.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create and inspect a bf Embedding\n",
    "emb_bf = bf.net.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "bf.matmul(emb_bf([2]), jnp.ones(shape=(3, 4))).backprop()\n",
    "print(emb_bf.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1664405705473/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 4., 4.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect grad for a non-pad token - note that the grad is the same for bf and torch!\n",
    "out = torch.matmul(emb(torch.tensor([2])), torch.ones(size=(3, 4), requires_grad=True))\n",
    "out.backward(gradient=torch.ones_like(out))\n",
    "assert(jnp.array_equal(emb_bf.weight.grad, emb.weight.grad))\n",
    "emb.weight.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 4., 4.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect grad for a pad token for torch - note that the grad DID NOT change from before in the 1st (pad token index) row\n",
    "out = torch.matmul(emb(torch.tensor([1])), torch.ones(size=(3, 4), requires_grad=True))\n",
    "out.backward(gradient=torch.ones_like(out))\n",
    "emb.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [4. 4. 4.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect grad for a pad token for bf, note that the grad DID NOT change from before in the pad token index. Same behavior as torch\n",
    "bf.matmul(emb_bf([1]), jnp.ones(shape=(3, 4))).backprop()\n",
    "print(emb_bf.weight.grad)\n",
    "assert(jnp.array_equal(emb_bf.weight.grad, emb.weight.grad.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf modules: [Embedding(5, 3, padding_idx=1)]\n",
      "torch modules: [Embedding(5, 3, padding_idx=1)]\n"
     ]
    }
   ],
   "source": [
    "# Check that bf and torch embedding contain the same modules \n",
    "print(\"bf modules:\", [i for i in emb_bf.modules()])\n",
    "print(\"torch modules:\", [i for i in emb.modules()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node(name: emb weights (5, 3), val: [[-0.716899   -0.20865498 -2.5713923 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.8396519   0.3010434   0.1421263 ]\n",
      " [-1.7631724  -1.6755073   0.31390068]\n",
      " [ 0.5912831   0.5325395  -0.9133108 ]], grad: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [4. 4. 4.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]])]\n",
      "[(Parameter containing:\n",
      "tensor([[ 0.0813,  1.1262,  0.3836],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-1.1691,  0.5396,  0.3151],\n",
      "        [-0.1995, -0.8717,  1.5982],\n",
      "        [-0.1942, -0.6943, -0.2199]], requires_grad=True), tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [4., 4., 4.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]))]\n"
     ]
    }
   ],
   "source": [
    "# Check that we can access parameters of both bf and torch embeddings\n",
    "print([i for i in emb_bf.parameters()])\n",
    "print([(i, i.grad) for i in emb.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check that buffers are the same between bf and torch embeddings\n",
    "print([i for i in emb_bf.buffers()])\n",
    "print([i for i in emb.buffers()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a torch embedding state dict and reloading it as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_path = \"emb.pt\"\n",
    "torch.save(emb.state_dict(), save_torch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_loaded = torch.nn.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "emb_loaded.load_state_dict(torch.load(save_torch_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node(name: emb weights (5, 3), val: [[ 0.08126468  1.1262476   0.38362706]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.1691363   0.5395742   0.3151424 ]\n",
      " [-0.19945197 -0.87169373  1.5982424 ]\n",
      " [-0.19424585 -0.69431293 -0.21993046]], grad: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0813,  1.1262,  0.3836],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-1.1691,  0.5396,  0.3151],\n",
      "        [-0.1995, -0.8717,  1.5982],\n",
      "        [-0.1942, -0.6943, -0.2199]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# check that bf embedding when loading from a torch state dict ends up with same values\n",
    "emb_bf_loaded = bf.net.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "emb_bf_loaded.load_state_dict(torch.load(save_torch_path))\n",
    "print(emb_bf_loaded.weight)\n",
    "print(emb_loaded.weight)\n",
    "assert(check_node_equals_tensor(emb_bf_loaded.weight, emb_loaded.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf output of emb: node(name: get_embedding, val: [[ 0.          0.          0.        ]\n",
      " [-1.1691363   0.5395742   0.3151424 ]\n",
      " [-0.19945197 -0.87169373  1.5982424 ]], grad: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]])\n",
      "torch output of emb: tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [-1.1691,  0.5396,  0.3151],\n",
      "        [-0.1995, -0.8717,  1.5982]], grad_fn=<EmbeddingBackward0>)\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]] tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Check forward and backward passes to make sure the outputs match\n",
    "inds = [1,2,3]\n",
    "embs_bf_out = emb_bf_loaded(jnp.array(inds))\n",
    "embs_out = emb_loaded(torch.tensor(inds))\n",
    "print(\"bf output of emb:\", embs_bf_out)\n",
    "print(\"torch output of emb:\", embs_out)\n",
    "assert(check_node_equals_tensor(embs_bf_out, embs_out))\n",
    "\n",
    "embs_bf_out.backprop()\n",
    "embs_out.backward(gradient=torch.ones_like(embs_out))\n",
    "# print(embs_bf_out.grad, embs_out.grad)\n",
    "assert(jnp.array_equal(emb_bf_loaded.weight.grad, emb_loaded.weight.grad.numpy()))\n",
    "print(emb_bf_loaded.weight.grad, emb_loaded.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "emb_bf_loaded = bf.net.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "emb_bf_loaded.load_state_dict(torch.load(save_torch_path))\n",
    "out = bf.get_embedding(emb_bf_loaded.weight, [0, 1, 2, 3, 4], padding_idx=1)\n",
    "out.backprop()\n",
    "print(emb_bf_loaded.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
