{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "import brunoflow as bf\n",
    "from brunoflow.ad.utils import check_node_equals_tensor, check_node_allclose_tensor\n",
    "from jax import numpy as jnp, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "random_key_val = 42\n",
    "num_embeddings = 5\n",
    "embedding_dim = 3\n",
    "padding_idx = 1\n",
    "random_key = random.PRNGKey(random_key_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2964,  0.1638, -1.2453],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [-0.3869, -1.0141,  0.5715],\n",
       "        [-1.1094, -0.8435,  0.0700],\n",
       "        [-0.8782, -1.0346,  0.3617]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and inspect a torch embedding\n",
    "import torch\n",
    "emb = torch.nn.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [4. 4. 4.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create and inspect a bf Embedding\n",
    "emb_bf = bf.net.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "bf.matmul(emb_bf(jnp.array([2])), jnp.ones(shape=(3, 4))).backprop()\n",
    "print(emb_bf.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 4., 4.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect grad for a non-pad token - note that the grad is the same for bf and torch!\n",
    "out = torch.matmul(emb(torch.tensor([2])), torch.ones(size=(3, 4), requires_grad=True))\n",
    "out.backward(gradient=torch.ones_like(out))\n",
    "assert(jnp.array_equal(emb_bf.weight.grad, emb.weight.grad))\n",
    "emb.weight.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [4., 4., 4.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect grad for a pad token for torch - note that the grad DID NOT change from before in the 1st (pad token index) row\n",
    "out = torch.matmul(emb(torch.tensor([1])), torch.ones(size=(3, 4), requires_grad=True))\n",
    "out.backward(gradient=torch.ones_like(out))\n",
    "emb.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [4. 4. 4.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect grad for a pad token for bf, note that the grad DID NOT change from before in the pad token index. Same behavior as torch\n",
    "bf.matmul(emb_bf(jnp.array([1])), jnp.ones(shape=(3, 4))).backprop()\n",
    "print(emb_bf.weight.grad)\n",
    "assert(jnp.array_equal(emb_bf.weight.grad, emb.weight.grad.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf modules: [Embedding(5, 3, padding_idx=1)]\n",
      "torch modules: [Embedding(5, 3, padding_idx=1)]\n"
     ]
    }
   ],
   "source": [
    "# Check that bf and torch embedding contain the same modules \n",
    "print(\"bf modules:\", [i for i in emb_bf.modules()])\n",
    "print(\"torch modules:\", [i for i in emb.modules()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node(name: emb weights (5, 3), val: [[-0.716899   -0.20865498 -2.5713923 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.8396519   0.3010434   0.1421263 ]\n",
      " [-1.7631724  -1.6755074   0.31390068]\n",
      " [ 0.5912831   0.5325395  -0.9133108 ]], grad: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [4. 4. 4.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]])]\n",
      "[(Parameter containing:\n",
      "tensor([[-0.2964,  0.1638, -1.2453],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.3869, -1.0141,  0.5715],\n",
      "        [-1.1094, -0.8435,  0.0700],\n",
      "        [-0.8782, -1.0346,  0.3617]], requires_grad=True), tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [4., 4., 4.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]))]\n"
     ]
    }
   ],
   "source": [
    "# Check that we can access parameters of both bf and torch embeddings\n",
    "print([i for i in emb_bf.parameters()])\n",
    "print([(i, i.grad) for i in emb.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check that buffers are the same between bf and torch embeddings\n",
    "print([i for i in emb_bf.buffers()])\n",
    "print([i for i in emb.buffers()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a torch embedding state dict and reloading it as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_torch_path = \"emb.pt\"\n",
    "torch.save(emb.state_dict(), save_torch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_loaded = torch.nn.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "emb_loaded.load_state_dict(torch.load(save_torch_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node(name: emb weights (5, 3), val: [[-0.2964264   0.1638094  -1.2453288 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.38688654 -1.0140668   0.5715288 ]\n",
      " [-1.1094499  -0.84354985  0.0699667 ]\n",
      " [-0.8781804  -1.0346296   0.3616861 ]], grad: [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]])\n",
      "Parameter containing:\n",
      "tensor([[-0.2964,  0.1638, -1.2453],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.3869, -1.0141,  0.5715],\n",
      "        [-1.1094, -0.8435,  0.0700],\n",
      "        [-0.8782, -1.0346,  0.3617]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# check that bf embedding when loading from a torch state dict ends up with same values\n",
    "emb_bf_loaded = bf.net.Embedding(num_embeddings=5, embedding_dim=3, padding_idx=1)\n",
    "emb_bf_loaded.load_state_dict(torch.load(save_torch_path))\n",
    "print(emb_bf_loaded.weight)\n",
    "print(emb_loaded.weight)\n",
    "assert(check_node_equals_tensor(emb_bf_loaded.weight, emb_loaded.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node(name: (getitem emb weights (5, 3) arg=[2 2 2]), val: [[-0.8396519  0.3010434  0.1421263]\n",
       " [-0.8396519  0.3010434  0.1421263]\n",
       " [-0.8396519  0.3010434  0.1421263]], grad: [[0. 0. 0.]\n",
       " [0. 0. 0.]\n",
       " [0. 0. 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_bf.weight[jnp.array([2,2,2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF and Torch equality tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1D ARRAY\n",
    "# Check forward and backward passes to make sure the outputs match\n",
    "inds = [0, 1, 2, 2, 3, 2]\n",
    "emb_bf_loaded.zero_grad()\n",
    "emb_loaded.zero_grad()\n",
    "embs_bf_out = emb_bf_loaded(bf.Node(jnp.array(inds), name=\"INDICES\"))\n",
    "embs_out = emb_loaded(torch.tensor(inds))\n",
    "\n",
    "embs_bf_out = bf.reduce_sum(embs_bf_out)\n",
    "embs_out = torch.sum(embs_out)\n",
    "\n",
    "print(\"bf output of emb:\", embs_bf_out)\n",
    "print(\"torch output of emb:\", embs_out)\n",
    "assert(check_node_allclose_tensor(embs_bf_out, embs_out))\n",
    "\n",
    "embs_bf_out.backprop(values_to_compute=(\"grad\",), verbose=False)\n",
    "embs_out.backward(gradient=torch.ones_like(embs_out))\n",
    "# print(embs_bf_out.grad, embs_out.grad)\n",
    "print(\"\\nbf grad:\", emb_bf_loaded.weight.grad)\n",
    "print(\"torch grad:\", emb_loaded.weight.grad.numpy())\n",
    "assert(jnp.array_equal(emb_bf_loaded.weight.grad, emb_loaded.weight.grad.numpy()))\n",
    "print(\"bf loaded and original grads:\", emb_bf_loaded.weight.grad, emb_loaded.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2D ARRAY\n",
    "# Check forward and backward passes to make sure the outputs match\n",
    "inds = [[0, 1, 2, 2, 2, 3], [2, 3, 4, 1, 1, 1]]\n",
    "emb_bf_loaded.zero_grad()\n",
    "emb_loaded.zero_grad()\n",
    "embs_bf_out = emb_bf_loaded(bf.Node(jnp.array(inds), name=\"INDICES\"))\n",
    "embs_out = emb_loaded(torch.tensor(inds))\n",
    "\n",
    "embs_bf_out = bf.reduce_sum(embs_bf_out)\n",
    "embs_out = torch.sum(embs_out)\n",
    "\n",
    "print(\"bf output of emb:\", embs_bf_out)\n",
    "print(\"torch output of emb:\", embs_out)\n",
    "assert(check_node_allclose_tensor(embs_bf_out, embs_out))\n",
    "\n",
    "embs_bf_out.backprop(values_to_compute=(\"grad\",), verbose=False)\n",
    "embs_out.backward(gradient=torch.ones_like(embs_out))\n",
    "# print(embs_bf_out.grad, embs_out.grad)\n",
    "print(\"\\nbf grad:\", emb_bf_loaded.weight.grad)\n",
    "print(\"torch grad:\", emb_loaded.weight.grad.numpy())\n",
    "assert(jnp.array_equal(emb_bf_loaded.weight.grad, emb_loaded.weight.grad.numpy()))\n",
    "print(\"bf loaded and original grads:\", emb_bf_loaded.weight.grad, emb_loaded.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf output of emb: node(name: (sum get_embedding axis=None keepdims=None), val: -5.749251842498779, grad: 0.0)\n",
      "torch output of emb: tensor(-5.7493, grad_fn=<SumBackward0>)\n",
      "\n",
      "bf grad: [[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [3. 3. 3.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "torch grad: [[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [3. 3. 3.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "bf loaded and original grads: [[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [3. 3. 3.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]] tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [3., 3., 3.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "### 2D ARRAY where one dim value is 1\n",
    "# Check forward and backward passes to make sure the outputs match\n",
    "inds = [[0, 1, 2, 2, 3, 2]]\n",
    "emb_bf_loaded.zero_grad()\n",
    "emb_loaded.zero_grad()\n",
    "embs_bf_out = emb_bf_loaded(bf.Node(jnp.array(inds), name=\"INDICES\"))\n",
    "embs_out = emb_loaded(torch.tensor(inds))\n",
    "\n",
    "embs_bf_out = bf.reduce_sum(embs_bf_out)\n",
    "embs_out = torch.sum(embs_out)\n",
    "\n",
    "print(\"bf output of emb:\", embs_bf_out)\n",
    "print(\"torch output of emb:\", embs_out)\n",
    "assert(check_node_allclose_tensor(embs_bf_out, embs_out))\n",
    "\n",
    "embs_bf_out.backprop(values_to_compute=(\"grad\",), verbose=False)\n",
    "embs_out.backward(gradient=torch.ones_like(embs_out))\n",
    "# print(embs_bf_out.grad, embs_out.grad)\n",
    "print(\"\\nbf grad:\", emb_bf_loaded.weight.grad)\n",
    "print(\"torch grad:\", emb_loaded.weight.grad.numpy())\n",
    "assert(jnp.array_equal(emb_bf_loaded.weight.grad, emb_loaded.weight.grad.numpy()))\n",
    "print(\"bf loaded and original grads:\", emb_bf_loaded.weight.grad, emb_loaded.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf output of emb: node(name: (sum get_embedding axis=None keepdims=None), val: -5.749251842498779, grad: 0.0)\n",
      "torch output of emb: tensor(-5.7493, grad_fn=<SumBackward0>)\n",
      "\n",
      "bf grad: [[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [3. 3. 3.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "torch grad: [[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [3. 3. 3.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "bf loaded and original grads: [[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [3. 3. 3.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]] tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [3., 3., 3.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "### 2D ARRAY where other dim value is 1\n",
    "# Check forward and backward passes to make sure the outputs match\n",
    "inds = jnp.array([[0, 1, 2, 2, 3, 2]]).transpose().__array__()\n",
    "emb_bf_loaded.zero_grad()\n",
    "emb_loaded.zero_grad()\n",
    "embs_bf_out = emb_bf_loaded(bf.Node(jnp.array(inds), name=\"INDICES\"))\n",
    "embs_out = emb_loaded(torch.tensor(inds))\n",
    "\n",
    "embs_bf_out = bf.reduce_sum(embs_bf_out)\n",
    "embs_out = torch.sum(embs_out)\n",
    "\n",
    "print(\"bf output of emb:\", embs_bf_out)\n",
    "print(\"torch output of emb:\", embs_out)\n",
    "assert(check_node_allclose_tensor(embs_bf_out, embs_out))\n",
    "\n",
    "embs_bf_out.backprop(values_to_compute=(\"grad\",), verbose=False)\n",
    "embs_out.backward(gradient=torch.ones_like(embs_out))\n",
    "# print(embs_bf_out.grad, embs_out.grad)\n",
    "print(\"\\nbf grad:\", emb_bf_loaded.weight.grad)\n",
    "print(\"torch grad:\", emb_loaded.weight.grad.numpy())\n",
    "assert(jnp.array_equal(emb_bf_loaded.weight.grad, emb_loaded.weight.grad.numpy()))\n",
    "print(\"bf loaded and original grads:\", emb_bf_loaded.weight.grad, emb_loaded.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf output of emb: node(name: (sum get_embedding axis=None keepdims=None), val: -12.22020435333252, grad: 0.0)\n",
      "torch output of emb: tensor(-12.2202, grad_fn=<SumBackward0>)\n",
      "\n",
      "bf grad: [[2. 2. 2.]\n",
      " [0. 0. 0.]\n",
      " [5. 5. 5.]\n",
      " [2. 2. 2.]\n",
      " [1. 1. 1.]]\n",
      "torch grad: [[2. 2. 2.]\n",
      " [0. 0. 0.]\n",
      " [5. 5. 5.]\n",
      " [2. 2. 2.]\n",
      " [1. 1. 1.]]\n",
      "bf loaded and original grads: [[2. 2. 2.]\n",
      " [0. 0. 0.]\n",
      " [5. 5. 5.]\n",
      " [2. 2. 2.]\n",
      " [1. 1. 1.]] tensor([[2., 2., 2.],\n",
      "        [0., 0., 0.],\n",
      "        [5., 5., 5.],\n",
      "        [2., 2., 2.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "### 3D ARRAY\n",
    "# Check forward and backward passes to make sure the outputs match\n",
    "inds = [[0, 1, 2, 2, 2, 3], [2, 3, 4, 1, 1, 1], [1, 0, 1, 1, 2, 1]]\n",
    "emb_bf_loaded.zero_grad()\n",
    "emb_loaded.zero_grad()\n",
    "embs_bf_out = emb_bf_loaded(bf.Node(jnp.array(inds), name=\"INDICES\"))\n",
    "embs_out = emb_loaded(torch.tensor(inds))\n",
    "\n",
    "embs_bf_out = bf.reduce_sum(embs_bf_out)\n",
    "embs_out = torch.sum(embs_out)\n",
    "\n",
    "print(\"bf output of emb:\", embs_bf_out)\n",
    "print(\"torch output of emb:\", embs_out)\n",
    "assert(check_node_allclose_tensor(embs_bf_out, embs_out))\n",
    "\n",
    "embs_bf_out.backprop(values_to_compute=(\"grad\",), verbose=False)\n",
    "embs_out.backward(gradient=torch.ones_like(embs_out))\n",
    "# print(embs_bf_out.grad, embs_out.grad)\n",
    "print(\"\\nbf grad:\", emb_bf_loaded.weight.grad)\n",
    "print(\"torch grad:\", emb_loaded.weight.grad.numpy())\n",
    "assert(jnp.array_equal(emb_bf_loaded.weight.grad, emb_loaded.weight.grad.numpy()))\n",
    "print(\"bf loaded and original grads:\", emb_bf_loaded.weight.grad, emb_loaded.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
