{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Layer Normalization\n",
    "summary: >\n",
    " A PyTorch implementation/tutorial of layer normalization.\n",
    "---\n",
    "# Layer Normalization\n",
    "This is a [PyTorch](https://pytorch.org) implementation of\n",
    "[Layer Normalization](https://papers.labml.ai/paper/1607.06450).\n",
    "### Limitations of [Batch Normalization](../batch_norm/index.html)\n",
    "* You need to maintain running means.\n",
    "* Tricky for RNNs. Do you need different normalizations for each step?\n",
    "* Doesn't work with small batch sizes;\n",
    "large NLP models are usually trained with small batch sizes.\n",
    "* Need to compute means and variances across devices in distributed training.\n",
    "## Layer Normalization\n",
    "Layer normalization is a simpler normalization method that works\n",
    "on a wider range of settings.\n",
    "Layer normalization transforms the inputs to have zero mean and unit variance\n",
    "across the features.\n",
    "*Note that batch normalization fixes the zero mean and unit variance for each element.*\n",
    "Layer normalization does it for each batch across all elements.\n",
    "Layer normalization is generally used for NLP tasks.\n",
    "We have used layer normalization in most of the\n",
    "[transformer implementations](../../transformers/gpt/index.html).\n",
    "\"\"\"\n",
    "from typing import Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn, Size\n",
    "\n",
    "\n",
    "class PyLayerNorm(nn.Module):\n",
    "    r\"\"\"\n",
    "    ## Layer Normalization\n",
    "    Layer normalization $\\text{LN}$ normalizes the input $X$ as follows:\n",
    "    When input $X \\in \\mathbb{R}^{B \\times C}$ is a batch of embeddings,\n",
    "    where $B$ is the batch size and $C$ is the number of features.\n",
    "    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n",
    "    $$\\text{LN}(X) = \\gamma\n",
    "    \\frac{X - \\underset{C}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C}{Var}[X] + \\epsilon}}\n",
    "    + \\beta$$\n",
    "    When input $X \\in \\mathbb{R}^{L \\times B \\times C}$ is a batch of a sequence of embeddings,\n",
    "    where $B$ is the batch size, $C$ is the number of channels, $L$ is the length of the sequence.\n",
    "    $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$.\n",
    "    $$\\text{LN}(X) = \\gamma\n",
    "    \\frac{X - \\underset{C}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C}{Var}[X] + \\epsilon}}\n",
    "    + \\beta$$\n",
    "    When input $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ is a batch of image representations,\n",
    "    where $B$ is the batch size, $C$ is the number of channels, $H$ is the height and $W$ is the width.\n",
    "    This is not a widely used scenario.\n",
    "    $\\gamma \\in \\mathbb{R}^{C \\times H \\times W}$ and $\\beta \\in \\mathbb{R}^{C \\times H \\times W}$.\n",
    "    $$\\text{LN}(X) = \\gamma\n",
    "    \\frac{X - \\underset{C, H, W}{\\mathbb{E}}[X]}{\\sqrt{\\underset{C, H, W}{Var}[X] + \\epsilon}}\n",
    "    + \\beta$$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape: Union[int, List[int], Size], *,\n",
    "                 eps: float = 1e-5,\n",
    "                 elementwise_affine: bool = True):\n",
    "        \"\"\"\n",
    "        * `normalized_shape` $S$ is the shape of the elements (except the batch).\n",
    "         The input should then be\n",
    "         $X \\in \\mathbb{R}^{* \\times S[0] \\times S[1] \\times ... \\times S[n]}$\n",
    "        * `eps` is $\\epsilon$, used in $\\sqrt{Var[X] + \\epsilon}$ for numerical stability\n",
    "        * `elementwise_affine` is whether to scale and shift the normalized value\n",
    "        We've tried to use the same names for arguments as PyTorch `LayerNorm` implementation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Convert `normalized_shape` to `torch.Size`\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = torch.Size([normalized_shape])\n",
    "        elif isinstance(normalized_shape, list):\n",
    "            normalized_shape = torch.Size(normalized_shape)\n",
    "        assert isinstance(normalized_shape, torch.Size)\n",
    "\n",
    "        #\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        # Create parameters for $\\gamma$ and $\\beta$ for gain and bias\n",
    "        if self.elementwise_affine:\n",
    "            self.gain = nn.Parameter(torch.ones(normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        `x` is a tensor of shape `[*, S[0], S[1], ..., S[n]]`.\n",
    "        `*` could be any number of dimensions.\n",
    "         For example, in an NLP task this will be\n",
    "        `[seq_len, batch_size, features]`\n",
    "        \"\"\"\n",
    "        # Sanity check to make sure the shapes match\n",
    "        assert self.normalized_shape == x.shape[-len(self.normalized_shape):]\n",
    "\n",
    "        # The dimensions to calculate the mean and variance on\n",
    "        dims = [-(i + 1) for i in range(len(self.normalized_shape))]\n",
    "\n",
    "        # Calculate the mean of all elements;\n",
    "        # i.e. the means for each element $\\mathbb{E}[X]$\n",
    "        mean = x.mean(dim=dims, keepdim=True)\n",
    "        # Calculate the squared mean of all elements;\n",
    "        # i.e. the means for each element $\\mathbb{E}[X^2]$\n",
    "        mean_x2 = (x ** 2).mean(dim=dims, keepdim=True)\n",
    "        # Variance of all element $Var[X] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2$\n",
    "        var = mean_x2 - mean ** 2\n",
    "\n",
    "        # Normalize $$\\hat{X} = \\frac{X - \\mathbb{E}[X]}{\\sqrt{Var[X] + \\epsilon}}$$\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # Scale and shift $$\\text{LN}(x) = \\gamma \\hat{X} + \\beta$$\n",
    "        if self.elementwise_affine:\n",
    "            x_norm = self.gain * x_norm + self.bias\n",
    "\n",
    "        #\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 10:32:31.541430: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import brunoflow as bf\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.normal(mean=0, std=1, size=(5, 10))\n",
    "x2 = x.clone().detach().requires_grad_(True)\n",
    "x3 = bf.Node(x.clone().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([ 1.2446905 , -4.288586  , -0.6201846 ,  4.830481  ,\n",
       "               3.4588838 , -0.7314156 , -1.2973553 , -5.1509047 ,\n",
       "               2.2078104 ,  0.34658068], dtype=float32),\n",
       " DeviceArray([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.], dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf_ln = bf.net.LayerNorm(10)\n",
    "bf_ln_out = bf_ln(x3)\n",
    "bf_ln_out.backprop()\n",
    "bf_ln.weight.grad, bf_ln.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/jax-hf/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1664405705473/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.2447, -4.2886, -0.6202,  4.8305,  3.4589, -0.7314, -1.2974, -5.1509,\n",
       "          2.2078,  0.3466]),\n",
       " tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_ln = PyLayerNorm(10)\n",
    "py_ln_out = py_ln(x)\n",
    "py_ln_out.backward(gradient=torch.ones_like(py_ln_out))\n",
    "py_ln.gain.grad, py_ln.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.2447, -4.2886, -0.6202,  4.8305,  3.4589, -0.7314, -1.2974, -5.1509,\n",
       "          2.2078,  0.3466]),\n",
       " tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = nn.LayerNorm(10)\n",
    "ln_out = ln(x2)\n",
    "ln_out.backward(gradient=torch.ones_like(ln_out))\n",
    "ln.weight.grad, ln.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import brunoflow as bf\n",
    "import torch\n",
    "# bf.func.reduce_mean(x.numpy(), axis=(-1,)).val\n",
    "node = bf.Node(torch.ones(size=(2,3,4)).numpy())\n",
    "s = bf.func.reduce_sum(node, axis=(1, 2), keepdims=False)\n",
    "# out = bf.matmul(s, bf.Node(torch.normal(mean=0, std=1, size=(4, 5)).numpy()))\n",
    "# print(node.shape, s.val.shape, out.val.shape)\n",
    "s.backprop()\n",
    "node.grad.shape\n",
    "s.grad\n",
    "# , torch.sum(x, dim=(0,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc9cc217af6b7e12b7da5c82d5884fde07a0e0f6b7f76767c2fbf53f076f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
