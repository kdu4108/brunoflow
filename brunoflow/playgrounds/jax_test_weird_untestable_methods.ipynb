{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
      "Running JAX on NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "import os\n",
    "import brunoflow as bf\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "import torch\n",
    "import unittest as ut\n",
    "print(f\"Running JAX on {jax.devices()[0].device_kind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brunoflow.test import utils\n",
    "inputs = utils.random_inputs([[(6, 5)]])\n",
    "fn=lambda x: x[1:3, 1]\n",
    "torch_fn=lambda x: x[1:3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<brunoflow.test.utils.TestInput at 0x7fe594902b50>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in inputs:\n",
    "    x = utils.inputs_to_params(inp)\n",
    "    x_t = utils.inputs_to_torch(inp)\n",
    "    y = fn(*x)\n",
    "    y_t = torch_fn(*x_t)\n",
    "    utils.check(ut.TestCase(), y, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = bf.Parameter(np.ones((10, 10)))\n",
    "np.zeros_like(a.val)\n",
    "\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([node(name: None, val: [[-0.45614302  1.92889087 -0.81652471  1.26533222  0.41356366]\n",
       "   [-0.65349289 -0.76481893 -0.19027144  0.28957672 -1.15048079]\n",
       "   [-0.14026162 -0.1134748   0.89817177 -0.37637173 -0.62206724]\n",
       "   [-1.30590398 -0.54389701 -2.14626584 -0.07253473  0.49106049]\n",
       "   [ 0.9034091   0.87678393  1.60258579  1.50054879 -1.07206867]\n",
       "   [-1.12693568  0.98924257 -2.08546269  0.41695462 -0.81677451]], grad: [[0. 0. 0. 0. 0.]\n",
       "   [0. 0. 0. 0. 0.]\n",
       "   [0. 0. 0. 0. 0.]\n",
       "   [0. 0. 0. 0. 0.]\n",
       "   [0. 0. 0. 0. 0.]\n",
       "   [0. 0. 0. 0. 0.]])],\n",
       " [tensor([[-0.4561,  1.9289, -0.8165,  1.2653,  0.4136],\n",
       "          [-0.6535, -0.7648, -0.1903,  0.2896, -1.1505],\n",
       "          [-0.1403, -0.1135,  0.8982, -0.3764, -0.6221],\n",
       "          [-1.3059, -0.5439, -2.1463, -0.0725,  0.4911],\n",
       "          [ 0.9034,  0.8768,  1.6026,  1.5005, -1.0721],\n",
       "          [-1.1269,  0.9892, -2.0855,  0.4170, -0.8168]], dtype=torch.float64)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _einsum_forward(subscripts: str, arr1, arr2):\n",
    "    \"\"\"Function to compute the einsum of two tensors of arbitrary shape.\"\"\"\n",
    "    return jnp.einsum(subscripts, arr1, arr2)\n",
    "\n",
    "def jax_einsum_backward_func(einsum_expr, A, B):\n",
    "    jac_A, jac_B = jax.jacfwd(_einsum_forward, argnums=(1, 2))(einsum_expr, A, B)\n",
    "    # print(jac_A.shape)\n",
    "    # print(jac_B.shape)\n",
    "    # print(jnp.einsum(\"ijk,ik->jk\", jac_A, jnp.ones_like(_einsum_forward(einsum_expr, A, B))))\n",
    "    A_grad = jnp.sum(jac_A, axis=tuple(range(len(jac_A.shape) - len(A.shape))))\n",
    "    B_grad = jnp.sum(jac_B, axis=tuple(range(len(jac_B.shape) - len(B.shape))))\n",
    "    \n",
    "    return A_grad, B_grad\n",
    "\n",
    "def check_backward_einsum(einsum_expr, A, B, jax_einsum_backward_func):\n",
    "    torch_A = torch.tensor(np.array(A), requires_grad=True)\n",
    "    torch_B = torch.tensor(np.array(B), requires_grad=True)\n",
    "    out = torch.einsum(einsum_expr, torch_A, torch_B)\n",
    "    out.backward(torch.ones_like(out))\n",
    "\n",
    "    A_jax_grad, B_jax_grad = jax_einsum_backward_func(einsum_expr, A, B)\n",
    "    \n",
    "    assert np.array_equal(A_jax_grad, torch_A.grad), f\"A_jax_grad value does not equal expected torch_A.grad value.\\nExpected: {torch_A.grad}\\nActual:{A_jax_grad}\"\n",
    "    assert np.array_equal(B_jax_grad, torch_B.grad), f\"B_jax_grad value does not equal expected torch_B.grad value.\\nExpected: {torch_B.grad}\\nActual:{B_jax_grad}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.ones(shape=(5, 10))\n",
    "B = jnp.ones(shape=(10, 2))\n",
    "einsum_expr = \"ij,jk->ik\"\n",
    "check_backward_einsum(einsum_expr, A, B, jax_einsum_backward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.ones(shape=(5, 10))\n",
    "B = jnp.ones(shape=(10, 2))\n",
    "einsum_expr = \"ij,jk->i\"\n",
    "check_backward_einsum(einsum_expr, A, B, jax_einsum_backward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = jnp.ones(shape=(5, 10, 2))\n",
    "B = jnp.ones(shape=(10, 2, 3, 3))\n",
    "einsum_expr = \"ijk,jkll->ikl\"\n",
    "check_backward_einsum(einsum_expr, A, B, jax_einsum_backward_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, h, l, r, d = 3, 2, 1, 9, 7\n",
    "A = np.random.normal(size=(b, h, l, d))\n",
    "B = np.random.normal(size=(l, r, d))\n",
    "einsum_expr = \"bhld,lrd->bhlr\"\n",
    "check_backward_einsum(einsum_expr, A, B, jax_einsum_backward_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_einsum_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchained_einsum\u001b[39m(A, B, C, einsum_expr1, einsum_expr2):\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m _einsum_forward(einsum_expr2, _einsum_forward(einsum_expr1, A, B), C)\n\u001b[0;32m---> 10\u001b[0m jac_A, jac_B \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mjacfwd(chained_einsum, argnums\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))(A, B, C, einsum_expr1, einsum_expr2)\n\u001b[1;32m     11\u001b[0m A_jax_grad \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msum(jac_A, axis\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(jac_A\u001b[39m.\u001b[39mshape) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(A\u001b[39m.\u001b[39mshape))))\n\u001b[1;32m     12\u001b[0m B_jax_grad \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msum(jac_B, axis\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(jac_B\u001b[39m.\u001b[39mshape) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(B\u001b[39m.\u001b[39mshape))))\n",
      "File \u001b[0;32m~/miniconda3/envs/jax/lib/python3.9/site-packages/jax/_src/api.py:1288\u001b[0m, in \u001b[0;36mjacfwd.<locals>.jacfun\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_aux:\n\u001b[1;32m   1287\u001b[0m   pushfwd \u001b[39m=\u001b[39m partial(_jvp, f_partial, dyn_args)\n\u001b[0;32m-> 1288\u001b[0m   y, jac \u001b[39m=\u001b[39m vmap(pushfwd, out_axes\u001b[39m=\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))(_std_basis(dyn_args))\n\u001b[1;32m   1289\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1290\u001b[0m   pushfwd \u001b[39m=\u001b[39m partial(_jvp, f_partial, dyn_args, has_aux\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [9], line 8\u001b[0m, in \u001b[0;36mchained_einsum\u001b[0;34m(A, B, C, einsum_expr1, einsum_expr2)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchained_einsum\u001b[39m(A, B, C, einsum_expr1, einsum_expr2):\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mreturn\u001b[39;00m _einsum_forward(einsum_expr2, _einsum_forward(einsum_expr1, A, B), C)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_einsum_forward' is not defined"
     ]
    }
   ],
   "source": [
    "A = jnp.ones(shape=(5, 10))\n",
    "B = jnp.ones(shape=(10, 2))\n",
    "C = jnp.ones(shape=(2, 4))\n",
    "einsum_expr1 = \"ij,jk->ik\"\n",
    "einsum_expr2 = \"ij,jk->i\"\n",
    "\n",
    "def chained_einsum(A, B, C, einsum_expr1, einsum_expr2):\n",
    "    return _einsum_forward(einsum_expr2, _einsum_forward(einsum_expr1, A, B), C)\n",
    "\n",
    "jac_A, jac_B = jax.jacfwd(chained_einsum, argnums=(0, 1))(A, B, C, einsum_expr1, einsum_expr2)\n",
    "A_jax_grad = jnp.sum(jac_A, axis=tuple(range(len(jac_A.shape) - len(A.shape))))\n",
    "B_jax_grad = jnp.sum(jac_B, axis=tuple(range(len(jac_B.shape) - len(B.shape))))\n",
    "\n",
    "torch_A = torch.tensor(np.array(A), requires_grad=True)\n",
    "torch_B = torch.tensor(np.array(B), requires_grad=True)\n",
    "torch_C = torch.tensor(np.array(C), requires_grad=True)\n",
    "out = torch.einsum(einsum_expr2, torch.einsum(einsum_expr1, torch_A, torch_B), torch_C)\n",
    "out.backward(torch.ones_like(out))\n",
    "\n",
    "assert np.array_equal(A_jax_grad, torch_A.grad)\n",
    "assert np.array_equal(B_jax_grad, torch_B.grad)\n",
    "# check_backward_einsum(einsum_expr, A, B, jax_einsum_backward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackprop(values_to_compute\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgrad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mabs_val_grad\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     18\u001b[0m \u001b[39m# print(bf_X_mm.grad)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(bf_X\u001b[39m.\u001b[39mgrad, bf_X_mm\u001b[39m.\u001b[39mgrad)\n\u001b[1;32m     21\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(bf_X\u001b[39m.\u001b[39mabs_val_grad, bf_X_mm\u001b[39m.\u001b[39mabs_val_grad)\n\u001b[1;32m     22\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(bf_X\u001b[39m.\u001b[39mentropy_wrt_output, bf_X_mm\u001b[39m.\u001b[39mentropy_wrt_output)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = jnp.ones(shape=(5, 10))\n",
    "W = jnp.ones(shape=(10, 1))\n",
    "y = jnp.ones(shape=(5, 1))\n",
    "einsum_expr = \"ij,jk->ik\"\n",
    "\n",
    "bf_X = bf.Node(X, name=\"X\")\n",
    "bf_W = bf.Node(W, name=\"W\")\n",
    "logits = bf.einsum(\"ij,jk->ik\", bf_X, bf_W)\n",
    "loss = bf.opt.mse_loss(logits, y)\n",
    "loss.backprop(values_to_compute=(\"grad\", \"abs_val_grad\", \"entropy\"))\n",
    "# print(bf_X.grad)\n",
    "\n",
    "bf_X_mm = bf.Node(X.copy(), name=\"X_mm\")\n",
    "bf_W_mm = bf.Node(W.copy(), name=\"W_mm\")\n",
    "logits = bf.matmul(bf_X_mm, bf_W_mm)\n",
    "loss = bf.opt.mse_loss(logits, y)\n",
    "loss.backprop(values_to_compute=(\"grad\", \"abs_val_grad\", \"entropy\"))\n",
    "# print(bf_X_mm.grad)\n",
    "\n",
    "assert np.array_equal(bf_X.grad, bf_X_mm.grad)\n",
    "assert np.array_equal(bf_X.abs_val_grad, bf_X_mm.abs_val_grad)\n",
    "assert np.array_equal(bf_X.entropy_wrt_output, bf_X_mm.entropy_wrt_output)\n",
    "\n",
    "# torch_X = torch.tensor(np.array(X), requires_grad=True)\n",
    "# torch_W = torch.tensor(np.array(W), requires_grad=True)\n",
    "# torch_y = torch.tensor(np.array(y), requires_grad=True)\n",
    "# out = torch.nn.MSELoss()(torch.einsum(einsum_expr1, torch_X, torch_W), torch_y)\n",
    "# out.backward()\n",
    "# print(torch_X.grad)\n",
    "# # assert np.array_equal(A_jax_grad, torch_A.grad)\n",
    "# # assert np.array_equal(B_jax_grad, torch_B.grad)\n",
    "# # check_backward_einsum(einsum_expr, A, B, jax_einsum_backward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0833, 1.0833],\n",
      "        [1.0833, 1.0833],\n",
      "        [1.0833, 1.0833],\n",
      "        [1.0833, 1.0833],\n",
      "        [1.0833, 1.0833]])\n"
     ]
    }
   ],
   "source": [
    "# x = jnp.ones(shape=(5, 2))\n",
    "# out = bf.gelu(x)\n",
    "# out.backprop()\n",
    "\n",
    "x_torch = torch.tensor(np.array(x), requires_grad=True)\n",
    "out = torch.nn.functional.gelu(x_torch)\n",
    "out.backward(torch.ones_like(out))\n",
    "print(x_torch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1.0833155, 1.0833155],\n",
       "             [1.0833155, 1.0833155],\n",
       "             [1.0833155, 1.0833155],\n",
       "             [1.0833155, 1.0833155],\n",
       "             [1.0833155, 1.0833155]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = jnp.ones(shape=(5, 2))\n",
    "jac = jax.jacfwd(lambda x: jax.nn.gelu(x, False))(X)\n",
    "jnp.sum(jac, axis=tuple(range(len(jac.shape) - len(X.shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('jax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e140c34e0a267ffc2d97632a88da9d0788950c86e37f3cbf599f33bfe9d6bd4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
